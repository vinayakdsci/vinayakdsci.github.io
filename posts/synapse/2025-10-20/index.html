<!doctype html><html class="dark light" lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://vinayakdsci.github.io name=base><title>
            
                Synapse: Implementing a DNN library from scratch
            
        </title><meta content="Synapse: Implementing a DNN library from scratch" property=og:title><link href=https://vinayakdsci.github.io/fonts.css rel=stylesheet><script src=https://vinayakdsci.github.io/js/codeblock.js></script><script src=https://vinayakdsci.github.io/js/note.js></script><script>MathJax = {
              tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
              }
            };</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link href=https://vinayakdsci.github.io/atom.xml rel=alternate title=vinayakdsci type=application/atom+xml><link href=https://vinayakdsci.github.io/theme/light.css rel=stylesheet><link href=https://vinayakdsci.github.io/theme/dark.css id=darkModeStyle rel=stylesheet><script src=https://vinayakdsci.github.io/js/themetoggle.js></script><script>setTheme(getSavedTheme());</script><link href=https://vinayakdsci.github.io/main.css media=screen rel=stylesheet><script src="https://vinayakdsci.github.io/js/searchElasticlunr.min.js?h=b6d784e207d8794b3e43" defer></script><body><div class=content><header><div class=main><a href=https://vinayakdsci.github.io>vinayakdsci</a><div class=socials><a class=social href=https://linkedin.com/in/vinayakdsci rel=me> <img alt=linkedin src=https://vinayakdsci.github.io/icons/social/linkedin.svg> </a><a class=social href=https://github.com/vinayakdsci/ rel=me> <img alt=github src=https://vinayakdsci.github.io/icons/social/github.svg> </a></div></div><nav><a href=https://vinayakdsci.github.io/about style=margin-left:.25em>About </a><a href=https://vinayakdsci.github.io/posts style=margin-left:.25em>Posts </a><button title="$SHORTCUT to open search" class=search-button id=search-button><img alt=Search class=search-icon src=https://vinayakdsci.github.io/icons/search.svg></button><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><div id=modal-content><h1 class=page-header id=modalTitle>Search</h1><div id=searchBar><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search... role=combobox spellcheck=false><button title="Clear search" class=clear-button id=clear-search><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></button></div><div id=results-container><div id=results-info><span id=zero_results style=display:none>No results</span><span id=one_result style=display:none>1 result</span><span id=many_results style=display:none>$NUMBER results</span></div><div id=results role=listbox></div></div></div></div><a onclick="toggleTheme(); event.preventDefault();" href=# id=dark-mode-toggle> <img alt=Light id=sun-icon src=https://vinayakdsci.github.io/icons/sun.svg style=filter:invert()> <img alt=Dark id=moon-icon src=https://vinayakdsci.github.io/icons/moon.svg> </a><script>updateItemToggleTheme()</script></nav></header><main><article><div class=title><div class=page-header>Synapse: Implementing a DNN library from scratch<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2025-10-20</time> :: 515 Words</div></div><section class=body><h1 id=a-deep-learning-framework-you-can-hold-in-your-head>A Deep Learning Framework You Can Hold in Your Head</h1><p>There’s something oddly satisfying about rebuilding the things we use every day.<p>We, as engineers, work with deep-learning stacks all the time - <strong>PyTorch</strong>, <strong>TensorFlow</strong>, <strong>JAX</strong>, <strong>Triton</strong> - yet we rarely pause to ask <em>how</em> they actually work under the hood.<p>So I decided to peel back all the abstractions - no GPUs, no kernels, no runtime magic - and rebuild the <strong>core ideas</strong> from scratch: <strong>tensors, computation graphs, and the chain rule</strong>. These are the building blocks that form the heart of every modern DNN library.<p>And that’s how <strong>Synapse</strong> began: a lean, CPU-only (for now) deep-learning runtime that fits comfortably in a few hundred lines of Python and NumPy.<p>The goal isn’t to reinvent PyTorch. It’s to <em>see the entire machine working at once</em> - every gear, every gradient.<p>When you call <code>loss.backward()</code>, there’s a quiet symphony unfolding beneath the surface: contexts capture intermediate states, the computation graph gets topologically sorted, gradients ripple backward through nodes, and broadcasting semantics tie everything neatly together.<p>The idea behind <strong>Synapse</strong> is to build that entire picture - end to end - in the simplest way possible, while still preserving the correctness and semantics you’d expect from libraries like PyTorch or JAX.<h1 id=the-heartbeat-of-synapse>The Heartbeat of <strong>Synapse</strong></h1><p>At the center of Synapse is the autograd engine. Every tensor knows how it was created through a tiny structure called <code>grad_fn</code>. That node links to the operation that produced it - <code>Add</code>, <code>Matmul</code>, <code>ReLU</code> - and carries a small <code>context</code> object that remembers just enough from the forward pass to reconstruct its gradient later.<p>During the backward pass, the engine performs a simple topological sort of all connected nodes and then walks them in reverse. Each node calls its own <code>backward</code> function, using what it saved earlier in the context. It’s pure chain rule, expressed in code.<p>Here’s the satisfying part: once you write those 30–40 lines that make this work, you can literally see gradients flow through your graph like water. <code>x + y → z → sum() → .backward()</code> Each node lights up in reverse order, accumulating its partials as it goes.<figure><p align=center><img alt="Alt text" src=../ComputeGraphSimpleContrast.png style=border:none width=600><figcaption style=text-align:center;font-size:x-small>Fig: A simple compute graph of an elementwise addtion of two tensors producing loss function L. Green arrows represent forward pass. Red arrows represent backward pass.</figcaption></figure><p>The figure above is a simple demonstration of compute graph. When the backward pass starts, it goes from the loss function $L$ back to the nodes that created it, traversing $Z$, the addition node and then its inputs, $X$ and $Y$. Gradient computation takes place through the chain-rule, where the gradient of $L$ w.r.t $Z$ is used to compute gradients of $L$ w.r.t $X$ and $Y$.<h1 id=the-next-steps>The Next Steps</h1><p>This post was more of an announcement/introduction post. In the next post, we shall go over the <em>mathematics</em> for backward propagation, covering all the fundamental principles that we'll use as we contruct the <strong>autodiff</strong> engine for Synapse.<p>Till then, sit tight!</section></article></main></div>